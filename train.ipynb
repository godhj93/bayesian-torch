{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm import tqdm\n",
    "\n",
    "from models import LeNet_BNN, LeNet_BNN_uni, LeNet\n",
    "from utils import train_BNN, train_DNN, test_BNN, test_DNN\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "dnn = LeNet()\n",
    "dnn.load_state_dict(torch.load('runs/dnn_bs1024_lr0.001_mc100_temp_1.0_ep100_20240711-160323/best_model.pth'))\n",
    "\n",
    "# MNIST dataset\n",
    "train_dataset = datasets.MNIST(root='./data/', train=True, transform=transforms.ToTensor(), download=True)\n",
    "test_dataset = datasets.MNIST(root='./data/', train=False, transform=transforms.ToTensor())\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=1024, shuffle=True, num_workers=4, pin_memory=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=1024, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "test_DNN(dnn, test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bnn_uni = LeNet_BNN_uni().to(device)\n",
    "# bnn_uni.load_state_dict(torch.load('bnn_uni.pth'))\n",
    "# # train_BNN(10, bnn_uni)\n",
    "# test_BNN(bnn_uni, test_loader, 100, 1024, 'cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\mu$, $\\sigma$ 학습 가능한지 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.distributions import Normal\n",
    "target_w = 3.0\n",
    "\n",
    "# Sample the weights\n",
    "mu = torch.nn.Parameter(torch.Tensor(1))\n",
    "sigma = torch.nn.Parameter(torch.Tensor(1))\n",
    "mu.data.normal_(0, 0.1)\n",
    "sigma.data.normal_(1, 0.1)\n",
    "\n",
    "w_dist = Normal(mu,sigma)\n",
    "\n",
    "loss = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam([mu, sigma], lr=0.01)\n",
    "\n",
    "# Log for plotting\n",
    "mu_log = []\n",
    "sigma_log = []\n",
    "\n",
    "# Sample the weights\n",
    "for epoch in range(1000):\n",
    "    w = w_dist.rsample()\n",
    "    \n",
    "    loss_val = loss(w, torch.Tensor([target_w]))\n",
    "    optimizer.zero_grad()\n",
    "    loss_val.backward()\n",
    "    optimizer.step()\n",
    "    mu_log.append(mu.item())\n",
    "    sigma_log.append(sigma.item())\n",
    "    \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(mu_log)\n",
    "plt.plot(sigma_log)\n",
    "plt.grid()\n",
    "plt.legend(['mu', 'sigma'])\n",
    "\n",
    "# plot target\n",
    "plt.plot([0,1000], [target_w, target_w], 'r--')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 신경망 모델의 가중치를 분포로 학습가능한지 확인 $w_i \\sim \\mathbf{N}(\\mu_i,\\sigma_i)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "w_conv1_dnn = dnn.conv1.weight.data\n",
    "\n",
    "# Sample the weights\n",
    "mu = torch.nn.Parameter(torch.zeros_like(w_conv1_dnn))\n",
    "sigma = torch.nn.Parameter(torch.ones_like(w_conv1_dnn))\n",
    "\n",
    "w_dist = Normal(mu,sigma)\n",
    "\n",
    "loss = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam([mu, sigma], lr=0.01)\n",
    "\n",
    "# Log for plotting\n",
    "mu_log = []\n",
    "sigma_log = []\n",
    "losses = []\n",
    "for epoch in tqdm(range(1000)):\n",
    "    w = w_dist.rsample()\n",
    "    \n",
    "    loss_val = loss(w, w_conv1_dnn)\n",
    "    optimizer.zero_grad()\n",
    "    loss_val.backward()\n",
    "    optimizer.step()\n",
    "    mu_log.append(mu.mean().item())\n",
    "    sigma_log.append(sigma.mean().item())\n",
    "    losses.append(loss_val.item())\n",
    "    \n",
    "# plot training progress\n",
    "plt.plot(losses)\n",
    "plt.plot(mu_log)\n",
    "plt.plot(sigma_log)\n",
    "plt.grid()\n",
    "plt.legend(['loss', 'mu', 'sigma'])\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "\n",
    "# 예시 데이터, 실제 데이터로 대체하세요.\n",
    "mu_kernel = mu  # 실제 mu_kernel 값으로 대체\n",
    "rho_kernel = sigma  # 실제 rho_kernel 값으로 대체\n",
    "w_dnn = dnn.conv1.weight.data\n",
    "# sigma_weight 계산\n",
    "sigma_weight = torch.log1p(torch.exp(rho_kernel))\n",
    "\n",
    "plt.figure(figsize=(15,15))\n",
    "for idx, (w_target, mu, sigma) in enumerate(zip(w_dnn, mu_kernel, sigma_weight)):\n",
    "    \n",
    "    print(w_target.shape, mu.shape, sigma.shape)\n",
    "    w_target = w_target.cpu().detach().numpy()\n",
    "    plt.subplot(6,1, idx+1)\n",
    "    for i in range(3):\n",
    "        \n",
    "        for j in range(3):\n",
    "            \n",
    "            plt.axvline(x=w_target[0,i,j], color='r', linestyle='--', label='target')\n",
    "            \n",
    "            x = np.linspace(mu[0,i,j].item() - 3 * sigma[0,i,j].item(), mu[0,i,j].item() + 3 * sigma[0,i,j].item(), 1000)\n",
    "            pdf = norm.pdf(x, loc=mu[0,i,j].item(), scale= sigma[0,i,j].item())\n",
    "            # plt.subplot(3,3,i+j+1)\n",
    "\n",
    "            plt.plot(x, pdf, label=f'N({mu[0,i,j].item():.2f}, {sigma[0,i,j].item():.2f})')\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 만약 입력 X까지 고려한 출력 y를 loss로 사용한다면?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bnn_uni = LeNet_BNN_uni().to(device)\n",
    "# loss = torch.nn.MSELoss()\n",
    "# optimizer = torch.optim.Adam(bnn_uni.conv1.parameters(), lr=0.01)\n",
    "\n",
    "# losses = []\n",
    "\n",
    "# for e in range(10):\n",
    "#     for x,y in tqdm(train_loader):\n",
    "        \n",
    "#         y_bnn, _ = bnn_uni.conv1.forward(x.to(device))\n",
    "        \n",
    "            \n",
    "#         y_dnn = dnn.conv1.forward(x.to(device))\n",
    "\n",
    "#         loss_val = loss(y_bnn, y_dnn)\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "#         loss_val.backward()\n",
    "#         optimizer.step()\n",
    "        \n",
    "#         losses.append(loss_val.item())\n",
    "    \n",
    "\n",
    "\n",
    "# plt.plot(losses)\n",
    "# plt.grid()\n",
    "\n",
    "# import torch\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# from scipy.stats import norm\n",
    "\n",
    "# # 예시 데이터, 실제 데이터로 대체하세요.\n",
    "# mu_kernel = bnn_uni.conv1.mu_kernel  # 실제 mu_kernel 값으로 대체\n",
    "# rho_kernel = bnn_uni.conv1.rho_kernel  # 실제 rho_kernel 값으로 대체\n",
    "# w_dnn = dnn.conv1.weight.data\n",
    "# # sigma_weight 계산\n",
    "# sigma_weight = torch.log1p(torch.exp(rho_kernel))\n",
    "\n",
    "# plt.figure(figsize=(15,15))\n",
    "# for idx, (w_target, mu, sigma) in enumerate(zip(w_dnn, mu_kernel, sigma_weight)):\n",
    "    \n",
    "#     print(w_target.shape, mu.shape, sigma.shape)\n",
    "#     w_target = w_target.cpu().detach().numpy()\n",
    "#     plt.subplot(6,1, idx+1)\n",
    "#     for i in range(3):\n",
    "        \n",
    "#         for j in range(3):\n",
    "            \n",
    "#             plt.axvline(x=w_target[0,i,j], color='r', linestyle='--', label='target')\n",
    "            \n",
    "#             x = np.linspace(mu[0,i,j].item() - 3 * sigma[0,i,j].item(), mu[0,i,j].item() + 3 * sigma[0,i,j].item(), 1000)\n",
    "#             pdf = norm.pdf(x, loc=mu[0,i,j].item(), scale= sigma[0,i,j].item())\n",
    "#             # plt.subplot(3,3,i+j+1)\n",
    "\n",
    "#             plt.plot(x, pdf, label=f'N({mu[0,i,j].item():.2f}, {sigma[0,i,j].item():.2f})')\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aleatoric?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnn_uni = LeNet_BNN_uni().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "w_conv1_dnn = dnn.conv1.weight.data\n",
    "\n",
    "# Sample the weights\n",
    "\n",
    "# loss = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam([bnn_uni.conv1.mu_kernel, bnn_uni.conv1.rho_kernel], lr=1e-3)\n",
    "\n",
    "# Log for plotting\n",
    "mu_log = []\n",
    "sigma_log = []\n",
    "losses = []\n",
    "for epoch in range(1000):\n",
    "    \n",
    "    mu = bnn_uni.conv1.mu_kernel\n",
    "    sigma = torch.log1p(torch.exp(bnn_uni.conv1.rho_kernel))\n",
    "\n",
    "    loss_val = 0.5 * torch.mean((mu - w_conv1_dnn)**2 / sigma + sigma )\n",
    "        \n",
    "        \n",
    "    \n",
    "    # loss_val = loss(w, w_conv1_dnn)\n",
    "    optimizer.zero_grad()\n",
    "    loss_val.backward()\n",
    "    optimizer.step()\n",
    "    mu_log.append(mu.mean().item())\n",
    "    sigma_log.append(sigma.mean().item())\n",
    "    losses.append(loss_val.item())\n",
    "    \n",
    "# plot training progress\n",
    "plt.plot(losses)\n",
    "plt.plot(mu_log)\n",
    "plt.plot(sigma_log)\n",
    "plt.grid()\n",
    "plt.legend(['loss', 'mu', 'sigma'])\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "\n",
    "# 예시 데이터, 실제 데이터로 대체하세요.\n",
    "mu_kernel = bnn_uni.conv1.mu_kernel  # 실제 mu_kernel 값으로 대체\n",
    "rho_kernel = bnn_uni.conv1.rho_kernel  # 실제 rho_kernel 값으로 대체\n",
    "w_dnn = dnn.conv1.weight.data\n",
    "# sigma_weight 계산\n",
    "sigma_weight = torch.log1p(torch.exp(rho_kernel))\n",
    "\n",
    "plt.figure(figsize=(15,15))\n",
    "for idx, (w_target, mu, sigma) in enumerate(zip(w_dnn, mu_kernel, sigma_weight)):\n",
    "    \n",
    "    print(w_target.shape, mu.shape, sigma.shape)\n",
    "    w_target = w_target.cpu().detach().numpy()\n",
    "    plt.subplot(6,1, idx+1)\n",
    "    for i in range(3):\n",
    "        \n",
    "        for j in range(3):\n",
    "            \n",
    "            plt.axvline(x=w_target[0,i,j], color='r', linestyle='--', label='target')\n",
    "            \n",
    "            x = np.linspace(mu[0,i,j].item() - 3 * sigma[0,i,j].item(), mu[0,i,j].item() + 3 * sigma[0,i,j].item(), 1000)\n",
    "            pdf = norm.pdf(x, loc=mu[0,i,j].item(), scale= sigma[0,i,j].item())\n",
    "            # plt.subplot(3,3,i+j+1)\n",
    "\n",
    "            plt.plot(x, pdf, label=f'N({mu[0,i,j].item():.2f}, {sigma[0,i,j].item():.2f})')\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "w_conv2_dnn = dnn.conv2.weight.data\n",
    "\n",
    "# Sample the weights\n",
    "\n",
    "# loss = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam([bnn_uni.conv2.mu_kernel, bnn_uni.conv2.rho_kernel], lr=1e-3)\n",
    "\n",
    "# Log for plotting\n",
    "mu_log = []\n",
    "sigma_log = []\n",
    "losses = []\n",
    "for epoch in range(1000):\n",
    "    \n",
    "    mu = bnn_uni.conv2.mu_kernel\n",
    "    sigma = torch.log1p(torch.exp(bnn_uni.conv2.rho_kernel))\n",
    "\n",
    "    loss_val = 0.5 * torch.mean((mu - w_conv2_dnn)**2 / sigma + sigma )\n",
    "        \n",
    "        \n",
    "    \n",
    "    # loss_val = loss(w, w_conv2_dnn)\n",
    "    optimizer.zero_grad()\n",
    "    loss_val.backward()\n",
    "    optimizer.step()\n",
    "    mu_log.append(mu.mean().item())\n",
    "    sigma_log.append(sigma.mean().item())\n",
    "    losses.append(loss_val.item())\n",
    "    \n",
    "# plot training progress\n",
    "plt.plot(losses)\n",
    "plt.plot(mu_log)\n",
    "plt.plot(sigma_log)\n",
    "plt.grid()\n",
    "plt.legend(['loss', 'mu', 'sigma'])\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "\n",
    "# 예시 데이터, 실제 데이터로 대체하세요.\n",
    "mu_kernel = bnn_uni.conv2.mu_kernel  # 실제 mu_kernel 값으로 대체\n",
    "print(mu_kernel.shape)\n",
    "rho_kernel = bnn_uni.conv2.rho_kernel  # 실제 rho_kernel 값으로 대체\n",
    "w_dnn = dnn.conv2.weight.data\n",
    "# sigma_weight 계산\n",
    "sigma_weight = torch.log1p(torch.exp(rho_kernel))\n",
    "\n",
    "plt.figure(figsize=(15,15))\n",
    "for idx, (w_target, mu, sigma) in enumerate(zip(w_dnn, mu_kernel, sigma_weight)):\n",
    "    \n",
    "    print(w_target.shape, mu.shape, sigma.shape)\n",
    "    w_target = w_target.cpu().detach().numpy()\n",
    "    plt.subplot(16,1, idx+1)\n",
    "    for i in range(3):\n",
    "        \n",
    "        for j in range(3):\n",
    "            \n",
    "            plt.axvline(x=w_target[0,i,j], color='r', linestyle='--', label='target')\n",
    "            \n",
    "            x = np.linspace(mu[0,i,j].item() - 3 * sigma[0,i,j].item(), mu[0,i,j].item() + 3 * sigma[0,i,j].item(), 1000)\n",
    "            pdf = norm.pdf(x, loc=mu[0,i,j].item(), scale= sigma[0,i,j].item())\n",
    "            # plt.subplot(3,3,i+j+1)\n",
    "\n",
    "            plt.plot(x, pdf, label=f'N({mu[0,i,j].item():.2f}, {sigma[0,i,j].item():.2f})')\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the Prior distribution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnn_new = LeNet_BNN_uni().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnn_new.conv1.prior_mean, bnn_new.conv1.prior_variance, bnn_new.conv2.prior_mean, bnn_new.conv2.prior_variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bnn_new.conv1.prior_mean = bnn_uni.conv1.mu_kernel.detach().clone()\n",
    "# bnn_new.conv1.prior_variance = torch.log1p(torch.exp(bnn_uni.conv1.rho_kernel)).detach().clone()\n",
    "\n",
    "# bnn_new.conv2.prior_mean = bnn_uni.conv2.mu_kernel.detach().clone()\n",
    "# bnn_new.conv2.prior_variance = torch.log1p(torch.exp(bnn_uni.conv2.rho_kernel)).detach().clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnn_new.conv1.prior_weight_mu = bnn_uni.conv1.mu_kernel.detach().clone()\n",
    "bnn_new.conv1.prior_weight_sigma = torch.log1p(torch.exp(bnn_uni.conv1.rho_kernel)).detach().clone()\n",
    "\n",
    "bnn_new.conv2.prior_weight_mu = bnn_uni.conv2.mu_kernel.detach().clone()\n",
    "bnn_new.conv2.prior_weight_sigma = torch.log1p(torch.exp(bnn_uni.conv2.rho_kernel)).detach().clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch.optim as optim\n",
    "# from torch.utils.tensorboard import SummaryWriter\n",
    "# class args:\n",
    "#     pass\n",
    "# args = args()\n",
    "# args.t = 1.0\n",
    "# writer = SummaryWriter(f'runs/bnn_new2')\n",
    "# train_BNN(epoch= 100, model = bnn_new, train_loader= train_loader, test_loader= test_loader, optimizer= optim.Adam(bnn_new.parameters(), lr=1e-3), writer = writer, mc_runs = 100, bs = 1024, device = 'cuda', args=args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MOPED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from bayesian_torch.models.dnn_to_bnn import dnn_to_bnn, get_kl_loss\n",
    "# from torch.utils.tensorboard import SummaryWriter\n",
    "# bnn_rho_init = -3.0\n",
    "# moped_delta_factor = 0.2\n",
    "# const_bnn_prior_parameters = {\n",
    "#         \"prior_mu\": 0.0,\n",
    "#         \"prior_sigma\": 1.0,\n",
    "#         \"posterior_mu_init\": 0.0,\n",
    "#         \"posterior_rho_init\": bnn_rho_init,\n",
    "#         \"type\": \"Reparameterization\",  # Flipout or Reparameterization\n",
    "#         \"moped_enable\": True,  # initialize mu/sigma from the dnn weights\n",
    "#         \"moped_delta\": moped_delta_factor,\n",
    "#     }\n",
    "\n",
    "\n",
    "# dnn_moped = LeNet()\n",
    "# dnn_moped.load_state_dict(torch.load('runs/dnn_bs1024_lr0.001_mc100_temp_1.0_ep100_20240711-160323/best_model.pth'))\n",
    "\n",
    "# dnn_to_bnn(dnn_moped, const_bnn_prior_parameters)\n",
    "\n",
    "# class args:\n",
    "#     pass\n",
    "\n",
    "# args = args()\n",
    "# args.t = 1.0\n",
    "# writer = SummaryWriter(f'runs/dnn_moped2')\n",
    "# train_BNN(epoch= 100, model = dnn_moped.cuda(), train_loader= train_loader, test_loader= test_loader, optimizer= optim.Adam(dnn_moped.parameters(), lr=1e-3), writer = writer, mc_runs = 100, bs = 1024, device = 'cuda', args=args, moped=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### multivariate Gaussian distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### covariance matrix 학습 가능한지 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_matrix = torch.eye(10, requires_grad=True)\n",
    "\n",
    "L_init = torch.tril(torch.rand(10,10))\n",
    "L = nn.Parameter(L_init)\n",
    "\n",
    "optimizer = optim.Adam([L], lr=1e-2)\n",
    "\n",
    "losses = []\n",
    "for epoch in range(1000):\n",
    "    \n",
    "    loss = 0.5 * torch.sum((L.T @ L - target_matrix)**2)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    losses.append(loss.item())\n",
    "    \n",
    "plt.plot(losses)\n",
    "plt.grid()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow((L.T@L).detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "bnn_multi = LeNet_BNN().to(device)\n",
    "\n",
    "import copy\n",
    "cov_not_optimized = bnn_multi.conv1.get_covariance_matrix().cpu().detach().numpy()  \n",
    "print(cov_not_optimized.shape)\n",
    "optimizer = optim.Adam([bnn_multi.conv1.mu_kernel, bnn_multi.conv1.L_param], lr=1e-2)\n",
    "\n",
    "mu_log = []\n",
    "losses = []\n",
    "mean_loss = []\n",
    "for idx, _ in tqdm(enumerate(range(10000))):\n",
    "    w_conv1_dnn = dnn.conv1.weight.data.to(device)\n",
    "    c_in, c_out, k, _ = w_conv1_dnn.size()\n",
    "    \n",
    "    mu = bnn_multi.conv1.mu_kernel\n",
    "    mu_flat = mu.view(-1)\n",
    "    w_flat = w_conv1_dnn.view(-1)\n",
    "\n",
    "    epslion = 1e-6\n",
    "    cov = bnn_multi.conv1.get_covariance_matrix() + epslion * torch.eye(c_in * c_out * k * k).to(device)\n",
    "    \n",
    "    # Sampling the weight\n",
    "    w = torch.distributions.MultivariateNormal(mu_flat, cov).rsample().reshape(c_in, c_out, k, k)\n",
    "    \n",
    "    # k_value = torch.tensor(k, dtype=torch.float32).to(device)\n",
    "    # pi_value = torch.tensor(torch.pi, dtype=torch.float32).to(device)\n",
    "    # nnl = 0.5 * (k_value * torch.log(2 * pi_value) + torch.logdet(cov) + (w_flat - mu_flat).t() @ torch.inverse(cov) @ (w_flat - mu_flat))\n",
    "\n",
    "    nnl = (w - w_conv1_dnn).pow(2).mean()\n",
    "    optimizer.zero_grad()\n",
    "    nnl.backward()  # Do not use retain_graph=True\n",
    "    optimizer.step()\n",
    "\n",
    "    mu_log.append(mu.mean().item())\n",
    "    \n",
    "    losses.append(nnl.item())\n",
    "    \n",
    "    mean_loss.append(np.mean(losses))\n",
    "\n",
    "plt.figure(figsize=(15,15))\n",
    "plt.subplot(1,3,1)\n",
    "plt.imshow(cov_not_optimized[:9,:9])\n",
    "plt.title('Covariance matrix (not optimized)')\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "i=1\n",
    "plt.imshow(bnn_multi.conv1.get_covariance_matrix().cpu().detach().numpy()[:,:][:9*i,:9*i])\n",
    "plt.title('Covariance matrix (optimized)')\n",
    "plt.colorbar()\n",
    "plt.subplot(1,3,3)\n",
    "plt.plot(mean_loss[:])\n",
    "plt.grid()\n",
    "plt.title('Loss')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnn_multi.conv1.L_param.T.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sample the weights\n",
    "weight_dist = torch.distributions.MultivariateNormal(\n",
    "    bnn_multi.conv1.mu_kernel.view(-1),\n",
    "    bnn_multi.conv1.get_covariance_matrix() + epslion * torch.eye(c_in * c_out * k * k).to(device)\n",
    ")\n",
    "\n",
    "w_bnn = weight_dist.rsample().reshape(6, 1, 3, 3).view(-1)\n",
    "w_dnn = dnn.conv1.weight.data.view(-1)\n",
    "\n",
    "# Visualize the difference using vertical lines\n",
    "plt.figure(figsize=(10, 5))\n",
    "x = range(len(w_dnn.cpu().detach().numpy()))\n",
    "\n",
    "# # Plot DNN weights with thicker lines\n",
    "# plt.vlines(x, ymin=0, ymax=w_dnn.cpu().detach().numpy(), color='r', linewidth=3, label='DNN')\n",
    "\n",
    "# # Plot BNN weights with thinner lines\n",
    "# plt.vlines(x, ymin=0, ymax=w_bnn.cpu().detach().numpy(), color='b', linewidth=1, label='BNN')\n",
    "\n",
    "# Plot differences\n",
    "\n",
    "plt.vlines(x, ymin=0, ymax=(w_dnn.cpu().detach() - w_bnn.cpu().detach()).abs(), color='g', linewidth=1, label='DNN - BNN')\n",
    "\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.title('DNN and BNN Weight Differences (Learned)')\n",
    "plt.xlabel('Weight Index')\n",
    "plt.ylabel('Diff (abs)')\n",
    "plt.ylim(0,3)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cov randomly initialized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "bnn_multi = LeNet_BNN().to(device)\n",
    "\n",
    "import copy\n",
    "cov_not_optimized = bnn_multi.conv1.get_covariance_matrix().cpu().detach().numpy()  \n",
    "print(cov_not_optimized.shape)\n",
    "optimizer = optim.Adam([bnn_multi.conv1.mu_kernel], lr=1e-2)\n",
    "\n",
    "mu_log = []\n",
    "losses = []\n",
    "mean_loss = []\n",
    "for idx, _ in tqdm(enumerate(range(10000))):\n",
    "    w_conv1_dnn = dnn.conv1.weight.data.to(device)\n",
    "    c_in, c_out, k, _ = w_conv1_dnn.size()\n",
    "    \n",
    "    mu = bnn_multi.conv1.mu_kernel\n",
    "    mu_flat = mu.view(-1)\n",
    "    w_flat = w_conv1_dnn.view(-1)\n",
    "\n",
    "    epslion = 1e-6\n",
    "    cov = bnn_multi.conv1.get_covariance_matrix() + epslion * torch.eye(c_in * c_out * k * k).to(device)\n",
    "    \n",
    "    # Sampling the weight\n",
    "    w = torch.distributions.MultivariateNormal(mu_flat, cov).rsample().reshape(c_in, c_out, k, k)\n",
    "    \n",
    "    # k_value = torch.tensor(k, dtype=torch.float32).to(device)\n",
    "    # pi_value = torch.tensor(torch.pi, dtype=torch.float32).to(device)\n",
    "    # nnl = 0.5 * (k_value * torch.log(2 * pi_value) + torch.logdet(cov) + (w_flat - mu_flat).t() @ torch.inverse(cov) @ (w_flat - mu_flat))\n",
    "\n",
    "    nnl = (w - w_conv1_dnn).pow(2).mean()\n",
    "    optimizer.zero_grad()\n",
    "    nnl.backward()  # Do not use retain_graph=True\n",
    "    optimizer.step()\n",
    "\n",
    "    mu_log.append(mu.mean().item())\n",
    "    \n",
    "    losses.append(nnl.item())\n",
    "    \n",
    "    mean_loss.append(np.mean(losses))\n",
    "\n",
    "plt.figure(figsize=(15,15))\n",
    "plt.subplot(1,3,1)\n",
    "plt.imshow(cov_not_optimized[:9,:9])\n",
    "plt.title('Covariance matrix (not optimized)')\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "i=1\n",
    "plt.imshow(bnn_multi.conv1.get_covariance_matrix().cpu().detach().numpy()[:,:][:9*i,:9*i])\n",
    "plt.title('Covariance matrix (optimized)')\n",
    "plt.colorbar()\n",
    "plt.subplot(1,3,3)\n",
    "plt.plot(mean_loss[:])\n",
    "plt.grid()\n",
    "plt.title('Loss')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sample the weights\n",
    "weight_dist = torch.distributions.MultivariateNormal(\n",
    "    bnn_multi.conv1.mu_kernel.view(-1),\n",
    "    bnn_multi.conv1.get_covariance_matrix() + epslion * torch.eye(c_in * c_out * k * k).to(device)\n",
    ")\n",
    "\n",
    "w_bnn = weight_dist.rsample().reshape(6, 1, 3, 3).view(-1)\n",
    "w_dnn = dnn.conv1.weight.data.view(-1)\n",
    "\n",
    "# Visualize the difference using vertical lines\n",
    "plt.figure(figsize=(10, 5))\n",
    "x = range(len(w_dnn.cpu().detach().numpy()))\n",
    "\n",
    "# # Plot DNN weights with thicker lines\n",
    "# plt.vlines(x, ymin=0, ymax=w_dnn.cpu().detach().numpy(), color='r', linewidth=3, label='DNN')\n",
    "\n",
    "# # Plot BNN weights with thinner lines\n",
    "# plt.vlines(x, ymin=0, ymax=w_bnn.cpu().detach().numpy(), color='b', linewidth=1, label='BNN')\n",
    "\n",
    "# Plot differences\n",
    "\n",
    "plt.vlines(x, ymin=0, ymax=(w_dnn.cpu().detach() - w_bnn.cpu().detach()).abs(), color='r', linewidth=1, label='DNN - BNN')\n",
    "\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.title('DNN and BNN Weight Differences (Randomly Initialized)')\n",
    "plt.xlabel('Weight Index')\n",
    "plt.ylabel('Diff (abs)')\n",
    "plt.ylim(0,3)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cov = I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "bnn_multi = LeNet_BNN().to(device)\n",
    "\n",
    "import copy\n",
    "cov_not_optimized = bnn_multi.conv1.get_covariance_matrix().cpu().detach().numpy()  \n",
    "print(cov_not_optimized.shape)\n",
    "optimizer = optim.Adam([bnn_multi.conv1.mu_kernel, bnn_multi.conv1.L_param], lr=1e-2)\n",
    "\n",
    "mu_log = []\n",
    "losses = []\n",
    "mean_loss = []\n",
    "for idx, _ in tqdm(enumerate(range(10000))):\n",
    "    w_conv1_dnn = dnn.conv1.weight.data.to(device)\n",
    "    c_in, c_out, k, _ = w_conv1_dnn.size()\n",
    "    \n",
    "    mu = bnn_multi.conv1.mu_kernel\n",
    "    mu_flat = mu.view(-1)\n",
    "    w_flat = w_conv1_dnn.view(-1)\n",
    "\n",
    "    epslion = 1e-6\n",
    "    cov = bnn_multi.conv1.get_covariance_matrix() + epslion * torch.eye(c_in * c_out * k * k).to(device)\n",
    "    \n",
    "    # Sampling the weight\n",
    "    w = torch.distributions.MultivariateNormal(mu_flat, torch.eye(c_in * c_out * k * k).to(device)).rsample().reshape(c_in, c_out, k, k)\n",
    "    \n",
    "    # k_value = torch.tensor(k, dtype=torch.float32).to(device)\n",
    "    # pi_value = torch.tensor(torch.pi, dtype=torch.float32).to(device)\n",
    "    # nnl = 0.5 * (k_value * torch.log(2 * pi_value) + torch.logdet(cov) + (w_flat - mu_flat).t() @ torch.inverse(cov) @ (w_flat - mu_flat))\n",
    "\n",
    "    nnl = (w - w_conv1_dnn).pow(2).mean()\n",
    "    optimizer.zero_grad()\n",
    "    nnl.backward()  # Do not use retain_graph=True\n",
    "    optimizer.step()\n",
    "\n",
    "    mu_log.append(mu.mean().item())\n",
    "    \n",
    "    losses.append(nnl.item())\n",
    "    \n",
    "    mean_loss.append(np.mean(losses))\n",
    "\n",
    "plt.figure(figsize=(15,15))\n",
    "plt.subplot(1,3,1)\n",
    "plt.imshow(cov_not_optimized[:9,:9])\n",
    "plt.title('Covariance matrix (not optimized)')\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "i=1\n",
    "plt.imshow(bnn_multi.conv1.get_covariance_matrix().cpu().detach().numpy()[:,:][:9*i,:9*i])\n",
    "plt.title('Covariance matrix (optimized)')\n",
    "plt.colorbar()\n",
    "plt.subplot(1,3,3)\n",
    "plt.plot(mean_loss[:])\n",
    "plt.grid()\n",
    "plt.title('Loss')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sample the weights\n",
    "weight_dist = torch.distributions.MultivariateNormal(\n",
    "    bnn_multi.conv1.mu_kernel.view(-1),\n",
    "    torch.eye(c_in * c_out * k * k).to(device)\n",
    ")\n",
    "\n",
    "w_bnn = weight_dist.rsample().reshape(6, 1, 3, 3).view(-1)\n",
    "w_dnn = dnn.conv1.weight.data.view(-1)\n",
    "\n",
    "# Visualize the difference using vertical lines\n",
    "plt.figure(figsize=(10, 5))\n",
    "x = range(len(w_dnn.cpu().detach().numpy()))\n",
    "\n",
    "# # Plot DNN weights with thicker lines\n",
    "# plt.vlines(x, ymin=0, ymax=w_dnn.cpu().detach().numpy(), color='r', linewidth=3, label='DNN')\n",
    "\n",
    "# # Plot BNN weights with thinner lines\n",
    "# plt.vlines(x, ymin=0, ymax=w_bnn.cpu().detach().numpy(), color='b', linewidth=1, label='BNN')\n",
    "\n",
    "plt.vlines(x, ymin=0, ymax=(w_bnn - w_dnn).abs().cpu().detach().numpy(), color='b', linewidth=1, label='BNN')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.title('DNN and BNN Weight Differences (Cov = I)')\n",
    "plt.xlabel('Weight Index')\n",
    "plt.ylabel('Diff (Abs)')\n",
    "plt.ylim(0,3)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# bnn_multi_not_trained = LeNet_BNN().to(device)\n",
    "\n",
    "# # Sample the weights\n",
    "# weight_dist = torch.distributions.MultivariateNormal(\n",
    "#     bnn_multi_not_trained.conv1.mu_kernel.view(-1),\n",
    "#     bnn_multi_not_trained.conv1.get_covariance_matrix()\n",
    "# )\n",
    "\n",
    "# w_bnn = weight_dist.rsample().reshape(6, 1, 3, 3).view(-1)\n",
    "# w_dnn = dnn.conv1.weight.data.view(-1)\n",
    "\n",
    "# # Visualize the difference using vertical lines\n",
    "# plt.figure(figsize=(10, 5))\n",
    "# x = range(len(w_dnn.cpu().detach().numpy()))\n",
    "\n",
    "# # # Plot DNN weights with thicker lines\n",
    "# # plt.vlines(x, ymin=0, ymax=w_dnn.cpu().detach().numpy(), color='r', linewidth=3, label='DNN')\n",
    "\n",
    "# # # Plot BNN weights with thinner lines\n",
    "# # plt.vlines(x, ymin=0, ymax=w_bnn.cpu().detach().numpy(), color='b', linewidth=1, label='BNN')\n",
    "\n",
    "# # Plot differences\n",
    "# plt.vlines(x, ymin=0, ymax=(w_bnn - w_dnn).cpu().detach().numpy(), color='b', linewidth=1, label='BNN')\n",
    "\n",
    "# plt.legend()\n",
    "# plt.grid()\n",
    "# plt.title('DNN and BNN Weight Differences')\n",
    "# plt.xlabel('Weight Index')\n",
    "# plt.ylabel('Weight Value')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer 2개 일 때는 어떻게 될까?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leanred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "bnn_multi = LeNet_BNN().to(device)\n",
    "optimizer = optim.Adam(bnn_multi.parameters(), lr=1e-3)\n",
    "\n",
    "mu_log = []\n",
    "losses = []\n",
    "mean_loss = []\n",
    "for idx, _ in tqdm(enumerate(range(10000))):\n",
    "    w_conv1_dnn = dnn.conv1.weight.data.to(device)\n",
    "    w_conv2_dnn = dnn.conv2.weight.data.to(device)\n",
    "    \n",
    "    c_in_conv1, c_out_conv1, k_conv1, _ = w_conv1_dnn.size()\n",
    "    c_in_conv2, c_out_conv2, k_conv2, _ = w_conv2_dnn.size()\n",
    "    \n",
    "    mu_bnn_conv1 = bnn_multi.conv1.mu_kernel\n",
    "    mu_flat_conv1 = mu_bnn_conv1.view(-1)\n",
    "    \n",
    "    mu_bnn_conv2 = bnn_multi.conv2.mu_kernel\n",
    "    mu_flat_conv2 = mu_bnn_conv2.view(-1)\n",
    "    \n",
    "    w_flat_conv1 = w_conv1_dnn.view(-1)\n",
    "    w_flat_conv2 = w_conv2_dnn.view(-1)\n",
    "\n",
    "    epslion = 1e-6\n",
    "    cov_1 = bnn_multi.conv1.get_covariance_matrix() + epslion * torch.eye(c_in_conv1 * c_out_conv1 * k_conv1 * k_conv1).to(device)\n",
    "    cov_2 = bnn_multi.conv2.get_covariance_matrix() + epslion * torch.eye(c_in_conv2 * c_out_conv2 * k_conv2 * k_conv2).to(device)\n",
    "    \n",
    "    # Sampling the weight\n",
    "    w_bnn_conv1 = torch.distributions.MultivariateNormal(mu_flat_conv1, cov_1).rsample().reshape(c_in_conv1, c_out_conv1, k_conv1, k_conv1)\n",
    "    w_bnn_conv2 = torch.distributions.MultivariateNormal(mu_flat_conv2, cov_2).rsample().reshape(c_in_conv2, c_out_conv2, k_conv2, k_conv2)\n",
    "\n",
    "    nll = (w_bnn_conv1 - w_conv1_dnn).pow(2).mean()\n",
    "    nll += (w_bnn_conv2 - w_conv2_dnn).pow(2).mean()\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    nll.backward()  # Do not use retain_graph=True\n",
    "    optimizer.step()\n",
    "\n",
    "    mu_log.append(mu.mean().item())\n",
    "    \n",
    "    losses.append(nll.item())\n",
    "    \n",
    "    mean_loss.append(np.mean(losses))\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "# plt.subplot(1,3,1)\n",
    "# plt.imshow(cov_not_optimized[:9,:9])\n",
    "# plt.title('Covariance matrix (not optimized)')\n",
    "\n",
    "# plt.subplot(1,3,2)\n",
    "# i=1\n",
    "# plt.imshow(bnn_multi.conv1.get_covariance_matrix().cpu().detach().numpy()[:,:][:9*i,:9*i])\n",
    "# plt.title('Covariance matrix (optimized)')\n",
    "# plt.colorbar()\n",
    "# plt.subplot(1,3,3)\n",
    "plt.plot(mean_loss[:])\n",
    "plt.grid()\n",
    "plt.title('Loss')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sample the weights\n",
    "weight_dist_conv1 = torch.distributions.MultivariateNormal(\n",
    "    bnn_multi.conv1.mu_kernel.view(-1),\n",
    "    bnn_multi.conv1.get_covariance_matrix() +  epslion * torch.eye(c_in_conv1 * c_out_conv1 * k_conv1 * k_conv1).to(device)\n",
    ")\n",
    "\n",
    "w_bnn_conv1 = weight_dist_conv1.rsample().reshape(6, 1, 3, 3).view(-1)\n",
    "w_dnn_conv1 = dnn.conv1.weight.data.view(-1)\n",
    "\n",
    "weight_dist_conv2 = torch.distributions.MultivariateNormal(\n",
    "    bnn_multi.conv2.mu_kernel.view(-1),\n",
    "    bnn_multi.conv2.get_covariance_matrix() +  epslion * torch.eye(c_in_conv2 * c_out_conv2 * k_conv2 * k_conv2).to(device)\n",
    ")    \n",
    "\n",
    "w_bnn_conv2 = weight_dist_conv2.rsample().reshape(16, 6, 3, 3).view(-1)\n",
    "w_dnn_conv2 = dnn.conv2.weight.data.view(-1)\n",
    "\n",
    "# Visualize the difference using vertical lines\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.subplot(2,2,1)\n",
    "x = range(len(w_dnn_conv1.cpu().detach().numpy()))\n",
    "\n",
    "# Plot DNN weights with thicker lines\n",
    "plt.vlines(x, ymin=0, ymax=w_dnn_conv1.cpu().detach().numpy(), color='r', linewidth=3, label='DNN')\n",
    "\n",
    "# Plot BNN weights with thinner lines\n",
    "plt.vlines(x, ymin=0, ymax=w_bnn_conv1.cpu().detach().numpy(), color='b', linewidth=1, label='BNN')\n",
    "\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.title('DNN and BNN Weight Differences (Conv1)')\n",
    "plt.xlabel('Weight Index')\n",
    "plt.ylabel('Weight Value')\n",
    "\n",
    "plt.subplot(2,2,2)\n",
    "x = range(len(w_dnn_conv2.cpu().detach().numpy()))\n",
    "\n",
    "# Plot DNN weights with thicker lines\n",
    "plt.vlines(x, ymin=0, ymax=w_dnn_conv2.cpu().detach().numpy(), color='r', linewidth=3, label='DNN')\n",
    "\n",
    "# Plot BNN weights with thinner lines\n",
    "plt.vlines(x, ymin=0, ymax=w_bnn_conv2.cpu().detach().numpy(), color='b', linewidth=1, label='BNN')\n",
    "\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.title('DNN and BNN Weight Differences (Conv2)')\n",
    "plt.xlabel('Weight Index')\n",
    "plt.ylabel('Weight Value')\n",
    "\n",
    "plt.subplot(2,2,3)\n",
    "x = range(len(w_dnn_conv1.cpu().detach().numpy()))\n",
    "\n",
    "# Plot differences\n",
    "plt.vlines(x, ymin=0, ymax=(w_bnn_conv1 - w_dnn_conv1).abs().cpu().detach().numpy(), color='b', linewidth=1, label='BNN')\n",
    "\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.title('DNN and BNN Weight Differences (Conv1)')\n",
    "plt.xlabel('Weight Index')\n",
    "plt.ylabel('Diff (Abs)')\n",
    "plt.ylim(0,3)\n",
    "\n",
    "plt.subplot(2,2,4)\n",
    "x = range(len(w_dnn_conv2.cpu().detach().numpy()))\n",
    "\n",
    "# Plot differences\n",
    "plt.vlines(x, ymin=0, ymax=(w_bnn_conv2 - w_dnn_conv2).abs().cpu().detach().numpy(), color='b', linewidth=1, label='BNN')\n",
    "\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.title('DNN and BNN Weight Differences (Conv2)')\n",
    "plt.xlabel('Weight Index')\n",
    "plt.ylabel('Diff (Abs)')\n",
    "plt.ylim(0,3)\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cov = I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "bnn_multi = LeNet_BNN().to(device)\n",
    "optimizer = optim.Adam(bnn_multi.parameters(), lr=1e-3)\n",
    "\n",
    "mu_log = []\n",
    "losses = []\n",
    "mean_loss = []\n",
    "for idx, _ in tqdm(enumerate(range(10000))):\n",
    "    w_conv1_dnn = dnn.conv1.weight.data.to(device)\n",
    "    w_conv2_dnn = dnn.conv2.weight.data.to(device)\n",
    "    \n",
    "    c_in_conv1, c_out_conv1, k_conv1, _ = w_conv1_dnn.size()\n",
    "    c_in_conv2, c_out_conv2, k_conv2, _ = w_conv2_dnn.size()\n",
    "    \n",
    "    mu_bnn_conv1 = bnn_multi.conv1.mu_kernel\n",
    "    mu_flat_conv1 = mu_bnn_conv1.view(-1)\n",
    "    \n",
    "    mu_bnn_conv2 = bnn_multi.conv2.mu_kernel\n",
    "    mu_flat_conv2 = mu_bnn_conv2.view(-1)\n",
    "    \n",
    "    w_flat_conv1 = w_conv1_dnn.view(-1)\n",
    "    w_flat_conv2 = w_conv2_dnn.view(-1)\n",
    "\n",
    "    epslion = 1e-6\n",
    "    cov_1 = torch.eye(c_in_conv1 * c_out_conv1 * k_conv1 * k_conv1).to(device)\n",
    "    cov_2 = torch.eye(c_in_conv2 * c_out_conv2 * k_conv2 * k_conv2).to(device)\n",
    "    \n",
    "    # Sampling the weight\n",
    "    w_bnn_conv1 = torch.distributions.MultivariateNormal(mu_flat_conv1, cov_1).rsample().reshape(c_in_conv1, c_out_conv1, k_conv1, k_conv1)\n",
    "    w_bnn_conv2 = torch.distributions.MultivariateNormal(mu_flat_conv2, cov_2).rsample().reshape(c_in_conv2, c_out_conv2, k_conv2, k_conv2)\n",
    "\n",
    "    nll = (w_bnn_conv1 - w_conv1_dnn).pow(2).mean()\n",
    "    nll += (w_bnn_conv2 - w_conv2_dnn).pow(2).mean()\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    nll.backward()  # Do not use retain_graph=True\n",
    "    optimizer.step()\n",
    "\n",
    "    mu_log.append(mu.mean().item())\n",
    "    \n",
    "    losses.append(nll.item())\n",
    "    \n",
    "    mean_loss.append(np.mean(losses))\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "# plt.subplot(1,3,1)\n",
    "# plt.imshow(cov_not_optimized[:9,:9])\n",
    "# plt.title('Covariance matrix (not optimized)')\n",
    "\n",
    "# plt.subplot(1,3,2)\n",
    "# i=1\n",
    "# plt.imshow(bnn_multi.conv1.get_covariance_matrix().cpu().detach().numpy()[:,:][:9*i,:9*i])\n",
    "# plt.title('Covariance matrix (optimized)')\n",
    "# plt.colorbar()\n",
    "# plt.subplot(1,3,3)\n",
    "plt.plot(mean_loss[:])\n",
    "plt.grid()\n",
    "plt.title('Loss')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sample the weights\n",
    "weight_dist_conv1 = torch.distributions.MultivariateNormal(\n",
    "    bnn_multi.conv1.mu_kernel.view(-1),\n",
    "    torch.eye(c_in_conv1 * c_out_conv1 * k_conv1 * k_conv1).to(device)\n",
    ")\n",
    "\n",
    "w_bnn_conv1 = weight_dist_conv1.rsample().reshape(6, 1, 3, 3).view(-1)\n",
    "w_dnn_conv1 = dnn.conv1.weight.data.view(-1)\n",
    "\n",
    "weight_dist_conv2 = torch.distributions.MultivariateNormal(\n",
    "    bnn_multi.conv2.mu_kernel.view(-1),\n",
    "    torch.eye(c_in_conv2 * c_out_conv2 * k_conv2 * k_conv2).to(device)\n",
    ")    \n",
    "\n",
    "w_bnn_conv2 = weight_dist_conv2.rsample().reshape(16, 6, 3, 3).view(-1)\n",
    "w_dnn_conv2 = dnn.conv2.weight.data.view(-1)\n",
    "\n",
    "# Visualize the difference using vertical lines\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.subplot(2,2,1)\n",
    "x = range(len(w_dnn_conv1.cpu().detach().numpy()))\n",
    "\n",
    "# Plot DNN weights with thicker lines\n",
    "plt.vlines(x, ymin=0, ymax=w_dnn_conv1.cpu().detach().numpy(), color='r', linewidth=3, label='DNN')\n",
    "\n",
    "# Plot BNN weights with thinner lines\n",
    "plt.vlines(x, ymin=0, ymax=w_bnn_conv1.cpu().detach().numpy(), color='b', linewidth=1, label='BNN')\n",
    "\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.title('DNN and BNN Weight Differences (Conv1)')\n",
    "plt.xlabel('Weight Index')\n",
    "plt.ylabel('Weight Value')\n",
    "\n",
    "plt.subplot(2,2,2)\n",
    "x = range(len(w_dnn_conv2.cpu().detach().numpy()))\n",
    "\n",
    "# Plot DNN weights with thicker lines\n",
    "plt.vlines(x, ymin=0, ymax=w_dnn_conv2.cpu().detach().numpy(), color='r', linewidth=3, label='DNN')\n",
    "\n",
    "# Plot BNN weights with thinner lines\n",
    "plt.vlines(x, ymin=0, ymax=w_bnn_conv2.cpu().detach().numpy(), color='b', linewidth=1, label='BNN')\n",
    "\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.title('DNN and BNN Weight Differences (Conv2)')\n",
    "plt.xlabel('Weight Index')\n",
    "plt.ylabel('Weight Value')\n",
    "\n",
    "plt.subplot(2,2,3)\n",
    "x = range(len(w_dnn_conv1.cpu().detach().numpy()))\n",
    "\n",
    "# Plot differences\n",
    "plt.vlines(x, ymin=0, ymax=(w_bnn_conv1 - w_dnn_conv1).abs().cpu().detach().numpy(), color='b', linewidth=1, label='BNN')\n",
    "\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.title('DNN and BNN Weight Differences (Conv1)')\n",
    "plt.xlabel('Weight Index')\n",
    "plt.ylabel('Diff (Abs)')\n",
    "plt.ylim(0,3)\n",
    "\n",
    "plt.subplot(2,2,4)\n",
    "x = range(len(w_dnn_conv2.cpu().detach().numpy()))\n",
    "\n",
    "# Plot differences\n",
    "plt.vlines(x, ymin=0, ymax=(w_bnn_conv2 - w_dnn_conv2).abs().cpu().detach().numpy(), color='b', linewidth=1, label='BNN')\n",
    "\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.title('DNN and BNN Weight Differences (Conv2)')\n",
    "plt.xlabel('Weight Index')\n",
    "plt.ylabel('Diff (Abs)')\n",
    "plt.ylim(0,3)\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cov randomly initialized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "bnn_multi = LeNet_BNN().to(device)\n",
    "optimizer = optim.Adam([bnn_multi.conv1.mu_kernel, bnn_multi.conv2.mu_kernel], lr=1e-3)\n",
    "\n",
    "mu_log = []\n",
    "losses = []\n",
    "mean_loss = []\n",
    "for idx, _ in tqdm(enumerate(range(10000))):\n",
    "    w_conv1_dnn = dnn.conv1.weight.data.to(device)\n",
    "    w_conv2_dnn = dnn.conv2.weight.data.to(device)\n",
    "    \n",
    "    c_in_conv1, c_out_conv1, k_conv1, _ = w_conv1_dnn.size()\n",
    "    c_in_conv2, c_out_conv2, k_conv2, _ = w_conv2_dnn.size()\n",
    "    \n",
    "    mu_bnn_conv1 = bnn_multi.conv1.mu_kernel\n",
    "    mu_flat_conv1 = mu_bnn_conv1.view(-1)\n",
    "    \n",
    "    mu_bnn_conv2 = bnn_multi.conv2.mu_kernel\n",
    "    mu_flat_conv2 = mu_bnn_conv2.view(-1)\n",
    "    \n",
    "    w_flat_conv1 = w_conv1_dnn.view(-1)\n",
    "    w_flat_conv2 = w_conv2_dnn.view(-1)\n",
    "\n",
    "    epslion = 1e-6\n",
    "    cov_1 = bnn_multi.conv1.get_covariance_matrix()\n",
    "    cov_2 = bnn_multi.conv2.get_covariance_matrix()\n",
    "    \n",
    "    # Sampling the weight\n",
    "    w_bnn_conv1 = torch.distributions.MultivariateNormal(mu_flat_conv1, cov_1).rsample().reshape(c_in_conv1, c_out_conv1, k_conv1, k_conv1)\n",
    "    w_bnn_conv2 = torch.distributions.MultivariateNormal(mu_flat_conv2, cov_2).rsample().reshape(c_in_conv2, c_out_conv2, k_conv2, k_conv2)\n",
    "\n",
    "    nll = (w_bnn_conv1 - w_conv1_dnn).pow(2).mean()\n",
    "    nll += (w_bnn_conv2 - w_conv2_dnn).pow(2).mean()\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    nll.backward()  # Do not use retain_graph=True\n",
    "    optimizer.step()\n",
    "\n",
    "    mu_log.append(mu_bnn_conv1.mean().item() + mu_bnn_conv2.mean().item())\n",
    "    \n",
    "    losses.append(nll.item())\n",
    "    \n",
    "    mean_loss.append(np.mean(losses))\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "# plt.subplot(1,3,1)\n",
    "# plt.imshow(cov_not_optimized[:9,:9])\n",
    "# plt.title('Covariance matrix (not optimized)')\n",
    "\n",
    "# plt.subplot(1,3,2)\n",
    "# i=1\n",
    "# plt.imshow(bnn_multi.conv1.get_covariance_matrix().cpu().detach().numpy()[:,:][:9*i,:9*i])\n",
    "# plt.title('Covariance matrix (optimized)')\n",
    "# plt.colorbar()\n",
    "# plt.subplot(1,3,3)\n",
    "plt.plot(mean_loss[:])\n",
    "plt.grid()\n",
    "plt.title('Loss')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sample the weights\n",
    "weight_dist_conv1 = torch.distributions.MultivariateNormal(\n",
    "    bnn_multi.conv1.mu_kernel.view(-1),\n",
    "    bnn_multi.conv1.get_covariance_matrix() +  epslion * torch.eye(c_in_conv1 * c_out_conv1 * k_conv1 * k_conv1).to(device)\n",
    ")\n",
    "\n",
    "w_bnn_conv1 = weight_dist_conv1.rsample().reshape(6, 1, 3, 3).view(-1)\n",
    "w_dnn_conv1 = dnn.conv1.weight.data.view(-1)\n",
    "\n",
    "weight_dist_conv2 = torch.distributions.MultivariateNormal(\n",
    "    bnn_multi.conv2.mu_kernel.view(-1),\n",
    "    bnn_multi.conv2.get_covariance_matrix() +  epslion * torch.eye(c_in_conv2 * c_out_conv2 * k_conv2 * k_conv2).to(device)\n",
    ")    \n",
    "\n",
    "w_bnn_conv2 = weight_dist_conv2.rsample().reshape(16, 6, 3, 3).view(-1)\n",
    "w_dnn_conv2 = dnn.conv2.weight.data.view(-1)\n",
    "\n",
    "# Visualize the difference using vertical lines\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.subplot(2,2,1)\n",
    "x = range(len(w_dnn_conv1.cpu().detach().numpy()))\n",
    "\n",
    "# Plot DNN weights with thicker lines\n",
    "plt.vlines(x, ymin=0, ymax=w_dnn_conv1.cpu().detach().numpy(), color='r', linewidth=3, label='DNN')\n",
    "\n",
    "# Plot BNN weights with thinner lines\n",
    "plt.vlines(x, ymin=0, ymax=w_bnn_conv1.cpu().detach().numpy(), color='b', linewidth=1, label='BNN')\n",
    "\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.title('DNN and BNN Weight Differences (Conv1)')\n",
    "plt.xlabel('Weight Index')\n",
    "plt.ylabel('Weight Value')\n",
    "\n",
    "plt.subplot(2,2,2)\n",
    "x = range(len(w_dnn_conv2.cpu().detach().numpy()))\n",
    "\n",
    "# Plot DNN weights with thicker lines\n",
    "plt.vlines(x, ymin=0, ymax=w_dnn_conv2.cpu().detach().numpy(), color='r', linewidth=3, label='DNN')\n",
    "\n",
    "# Plot BNN weights with thinner lines\n",
    "plt.vlines(x, ymin=0, ymax=w_bnn_conv2.cpu().detach().numpy(), color='b', linewidth=1, label='BNN')\n",
    "\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.title('DNN and BNN Weight Differences (Conv2)')\n",
    "plt.xlabel('Weight Index')\n",
    "plt.ylabel('Weight Value')\n",
    "\n",
    "plt.subplot(2,2,3)\n",
    "x = range(len(w_dnn_conv1.cpu().detach().numpy()))\n",
    "\n",
    "# Plot differences\n",
    "plt.vlines(x, ymin=0, ymax=(w_bnn_conv1 - w_dnn_conv1).abs().cpu().detach().numpy(), color='b', linewidth=1, label='BNN')\n",
    "\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.title('DNN and BNN Weight Differences (Conv1)')\n",
    "plt.xlabel('Weight Index')\n",
    "plt.ylabel('Diff (Abs)')\n",
    "plt.ylim(0,3)\n",
    "\n",
    "plt.subplot(2,2,4)\n",
    "x = range(len(w_dnn_conv2.cpu().detach().numpy()))\n",
    "\n",
    "# Plot differences\n",
    "plt.vlines(x, ymin=0, ymax=(w_bnn_conv2 - w_dnn_conv2).abs().cpu().detach().numpy(), color='b', linewidth=1, label='BNN')\n",
    "\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.title('DNN and BNN Weight Differences (Conv2)')\n",
    "plt.xlabel('Weight Index')\n",
    "plt.ylabel('Diff (Abs)')\n",
    "plt.ylim(0,3)\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the Prior distribution using the learned covariance matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9848, 0.046528087556362153)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm import tqdm\n",
    "\n",
    "from models import LeNet_BNN, LeNet_BNN_uni, LeNet\n",
    "from utils import train_BNN, train_DNN, test_BNN, test_DNN\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "dnn = LeNet()\n",
    "dnn.load_state_dict(torch.load('runs/dnn_bs1024_lr0.001_mc100_temp_1.0_ep100_20240711-160323/best_model.pth'))\n",
    "\n",
    "# MNIST dataset\n",
    "train_dataset = datasets.MNIST(root='./data/', train=True, transform=transforms.ToTensor(), download=True)\n",
    "test_dataset = datasets.MNIST(root='./data/', train=False, transform=transforms.ToTensor())\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=1024, shuffle=True, num_workers=4, pin_memory=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=1024, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "test_DNN(dnn, test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9853, 0.04415414296090603)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dnn = LeNet().to(device)\n",
    "dnn.load_state_dict(torch.load('runs/dnn_bs1024_lr0.001_mc100_ep100_20240710-202543/best_model.pth'))\n",
    "\n",
    "test_DNN(dnn, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10000it [01:03, 158.22it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Loss')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "bnn_multi = LeNet_BNN().to(device)\n",
    "optimizer = optim.Adam(bnn_multi.parameters(), lr=1e-3)\n",
    "\n",
    "mu_log = []\n",
    "losses = []\n",
    "mean_loss = []\n",
    "for idx, _ in tqdm(enumerate(range(10000))):\n",
    "    w_conv1_dnn = dnn.conv1.weight.data.to(device)\n",
    "    w_conv2_dnn = dnn.conv2.weight.data.to(device)\n",
    "    \n",
    "    c_in_conv1, c_out_conv1, k_conv1, _ = w_conv1_dnn.size()\n",
    "    c_in_conv2, c_out_conv2, k_conv2, _ = w_conv2_dnn.size()\n",
    "    \n",
    "    mu_bnn_conv1 = bnn_multi.conv1.mu_kernel\n",
    "    mu_flat_conv1 = mu_bnn_conv1.view(-1)\n",
    "    \n",
    "    mu_bnn_conv2 = bnn_multi.conv2.mu_kernel\n",
    "    mu_flat_conv2 = mu_bnn_conv2.view(-1)\n",
    "    \n",
    "    w_flat_conv1 = w_conv1_dnn.view(-1)\n",
    "    w_flat_conv2 = w_conv2_dnn.view(-1)\n",
    "\n",
    "    epslion = 1e-5\n",
    "    cov_1 = bnn_multi.conv1.get_covariance_matrix() + epslion * torch.eye(c_in_conv1 * c_out_conv1 * k_conv1 * k_conv1).to(device)\n",
    "    cov_2 = bnn_multi.conv2.get_covariance_matrix() + epslion * torch.eye(c_in_conv2 * c_out_conv2 * k_conv2 * k_conv2).to(device)\n",
    "    \n",
    "    # Sampling the weight\n",
    "    w_bnn_conv1 = torch.distributions.MultivariateNormal(mu_flat_conv1, cov_1).rsample().reshape(c_in_conv1, c_out_conv1, k_conv1, k_conv1)\n",
    "    w_bnn_conv2 = torch.distributions.MultivariateNormal(mu_flat_conv2, cov_2).rsample().reshape(c_in_conv2, c_out_conv2, k_conv2, k_conv2)\n",
    "\n",
    "    nll = (w_bnn_conv1 - w_conv1_dnn).pow(2).mean()\n",
    "    nll += (w_bnn_conv2 - w_conv2_dnn).pow(2).mean()\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    nll.backward()  # Do not use retain_graph=True\n",
    "    optimizer.step()\n",
    "\n",
    "    mu_log.append(mu_flat_conv1.mean().item() + mu_flat_conv2.mean().item())\n",
    "    \n",
    "    losses.append(nll.item())\n",
    "    \n",
    "    mean_loss.append(np.mean(losses))\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "# plt.subplot(1,3,1)\n",
    "# plt.imshow(cov_not_optimized[:9,:9])\n",
    "# plt.title('Covariance matrix (not optimized)')\n",
    "\n",
    "# plt.subplot(1,3,2)\n",
    "# i=1\n",
    "# plt.imshow(bnn_multi.conv1.get_covariance_matrix().cpu().detach().numpy()[:,:][:9*i,:9*i])\n",
    "# plt.title('Covariance matrix (optimized)')\n",
    "# plt.colorbar()\n",
    "# plt.subplot(1,3,3)\n",
    "plt.plot(mean_loss[:])\n",
    "plt.grid()\n",
    "plt.title('Loss')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sample the weights\n",
    "weight_dist_conv1 = torch.distributions.MultivariateNormal(\n",
    "    bnn_multi.conv1.mu_kernel.view(-1),\n",
    "    bnn_multi.conv1.get_covariance_matrix() +  epslion * torch.eye(c_in_conv1 * c_out_conv1 * k_conv1 * k_conv1).to(device)\n",
    ")\n",
    "\n",
    "w_bnn_conv1 = weight_dist_conv1.rsample().reshape(6, 1, 3, 3).view(-1)\n",
    "w_dnn_conv1 = dnn.conv1.weight.data.view(-1)\n",
    "\n",
    "weight_dist_conv2 = torch.distributions.MultivariateNormal(\n",
    "    bnn_multi.conv2.mu_kernel.view(-1),\n",
    "    bnn_multi.conv2.get_covariance_matrix() +  epslion * torch.eye(c_in_conv2 * c_out_conv2 * k_conv2 * k_conv2).to(device)\n",
    ")    \n",
    "\n",
    "w_bnn_conv2 = weight_dist_conv2.rsample().reshape(16, 6, 3, 3).view(-1)\n",
    "w_dnn_conv2 = dnn.conv2.weight.data.view(-1)\n",
    "\n",
    "# Visualize the difference using vertical lines\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.subplot(2,2,1)\n",
    "x = range(len(w_dnn_conv1.cpu().detach().numpy()))\n",
    "\n",
    "# Plot DNN weights with thicker lines\n",
    "plt.vlines(x, ymin=0, ymax=w_dnn_conv1.cpu().detach().numpy(), color='r', linewidth=3, label='DNN')\n",
    "\n",
    "# Plot BNN weights with thinner lines\n",
    "plt.vlines(x, ymin=0, ymax=w_bnn_conv1.cpu().detach().numpy(), color='b', linewidth=1, label='BNN')\n",
    "\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.title('DNN and BNN Weight Differences (Conv1)')\n",
    "plt.xlabel('Weight Index')\n",
    "plt.ylabel('Weight Value')\n",
    "\n",
    "plt.subplot(2,2,2)\n",
    "x = range(len(w_dnn_conv2.cpu().detach().numpy()))\n",
    "\n",
    "# Plot DNN weights with thicker lines\n",
    "plt.vlines(x, ymin=0, ymax=w_dnn_conv2.cpu().detach().numpy(), color='r', linewidth=3, label='DNN')\n",
    "\n",
    "# Plot BNN weights with thinner lines\n",
    "plt.vlines(x, ymin=0, ymax=w_bnn_conv2.cpu().detach().numpy(), color='b', linewidth=1, label='BNN')\n",
    "\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.title('DNN and BNN Weight Differences (Conv2)')\n",
    "plt.xlabel('Weight Index')\n",
    "plt.ylabel('Weight Value')\n",
    "\n",
    "plt.subplot(2,2,3)\n",
    "x = range(len(w_dnn_conv1.cpu().detach().numpy()))\n",
    "\n",
    "# Plot differences\n",
    "plt.vlines(x, ymin=0, ymax=(w_bnn_conv1 - w_dnn_conv1).abs().cpu().detach().numpy(), color='b', linewidth=1, label='BNN')\n",
    "\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.title('DNN and BNN Weight Differences (Conv1)')\n",
    "plt.xlabel('Weight Index')\n",
    "plt.ylabel('Diff (Abs)')\n",
    "plt.ylim(0,3)\n",
    "\n",
    "plt.subplot(2,2,4)\n",
    "x = range(len(w_dnn_conv2.cpu().detach().numpy()))\n",
    "\n",
    "# Plot differences\n",
    "plt.vlines(x, ymin=0, ymax=(w_bnn_conv2 - w_dnn_conv2).abs().cpu().detach().numpy(), color='b', linewidth=1, label='BNN')\n",
    "\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.title('DNN and BNN Weight Differences (Conv2)')\n",
    "plt.xlabel('Weight Index')\n",
    "plt.ylabel('Diff (Abs)')\n",
    "plt.ylim(0,3)\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "if torch.any(torch.isnan(mu_flat_conv2)) or torch.any(torch.isinf(mu_flat_conv2)):\n",
    "    raise ValueError(\"mu_flat tensor contains NaN or Inf values\")\n",
    "\n",
    "\n",
    "if torch.any(torch.isnan(cov_2)) or torch.any(torch.isinf(cov_2)):\n",
    "    raise ValueError(\"covariance matrix contains NaN or Inf values\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_flat_conv2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "bnn_with_good_prior = LeNet_BNN().to(device) \n",
    "\n",
    "'''\n",
    "bnn_with_good_prior.conv1.prior_mean = bnn.conv1.mu_kernel.detach().clone().flatten()\n",
    "bnn_with_good_prior.conv1.prior_variance = bnn_with_good_prior.conv1.get_covariance_matrix(bnn.conv1.L_param.detach().clone(), bnn.conv1.B_param.detach().clone())\n",
    "\n",
    "    epslion = 1e-6\n",
    "    cov_1 = bnn_multi.conv1.get_covariance_matrix() + epslion * torch.eye(c_in_conv1 * c_out_conv1 * k_conv1 * k_conv1).to(device)\n",
    "    cov_2 = bnn_multi.conv2.get_covariance_matrix() + epslion * torch.eye(c_in_conv2 * c_out_conv2 * k_conv2 * k_conv2).to(device)\n",
    "    \n",
    "'''\n",
    "epslion = 1e-5\n",
    "\n",
    "bnn_with_good_prior.conv1.prior_mean = bnn_multi.conv1.mu_kernel.detach().clone().flatten()\n",
    "bnn_with_good_prior.conv1.prior_variance = bnn_multi.conv1.get_covariance_matrix().detach().clone() + epslion * torch.eye(c_in_conv1 * c_out_conv1 * k_conv1 * k_conv1).to(device)\n",
    "\n",
    "bnn_with_good_prior.conv2.prior_mean = bnn_multi.conv2.mu_kernel.detach().clone().flatten()\n",
    "bnn_with_good_prior.conv2.prior_variance = bnn_multi.conv2.get_covariance_matrix().detach().clone() + epslion * torch.eye(c_in_conv2 * c_out_conv2 * k_conv2 * k_conv2).to(device)\n",
    "\n",
    "bnn_with_good_prior.conv1.mu_kernel = bnn_multi.conv1.mu_kernel\n",
    "# bnn_with_good_prior.conv1.L_param = bnn_multi.conv1.L_param\n",
    "\n",
    "bnn_with_good_prior.conv2.mu_kernel = bnn_multi.conv2.mu_kernel\n",
    "# bnn_with_good_prior.conv2.L_param = bnn_multi.conv2.L_param\n",
    "\n",
    "bnn_with_good_prior.fc1 = dnn.fc1\n",
    "\n",
    "plt.subplot(1,3,1)\n",
    "plt.imshow(bnn_with_good_prior.conv1.prior_variance.cpu().detach().numpy())\n",
    "# plt.colorbar()\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "plt.imshow(bnn_with_good_prior.conv2.prior_variance.cpu().detach().numpy())\n",
    "\n",
    "# plt.subplot(1,3,3)\n",
    "# plt.imshow(bnn_with_good_prior.fc1.prior_variance.cpu().detach().numpy())\n",
    "# plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class args:\n",
    "    pass\n",
    "args = args()\n",
    "args.t = 1.0\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter(f'runs/bnn_with_good_prior_multi_new_implementation_1e-5')\n",
    "train_BNN(epoch= 100, model = bnn_with_good_prior.cuda(), train_loader= train_loader, test_loader= test_loader, optimizer= optim.Adam(bnn_with_good_prior.parameters(), lr=1e-3), writer = writer, mc_runs = 100, bs = 1024, device = 'cuda', args=args, moped=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mu_kernel 복사, Prior 설정 -> 즉, Cov 학습한것을 Prior로 설정한것을 제외하고 동일함 (24.7.13)\n",
    "### 결과 -> 매우 좋은 성능을 보임"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cov = I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "bnn_multi = LeNet_BNN().to(device)\n",
    "optimizer = optim.Adam(bnn_multi.parameters(), lr=1e-3)\n",
    "\n",
    "mu_log = []\n",
    "losses = []\n",
    "mean_loss = []\n",
    "for idx, _ in tqdm(enumerate(range(10000))):\n",
    "    w_conv1_dnn = dnn.conv1.weight.data.to(device)\n",
    "    w_conv2_dnn = dnn.conv2.weight.data.to(device)\n",
    "    \n",
    "    c_in_conv1, c_out_conv1, k_conv1, _ = w_conv1_dnn.size()\n",
    "    c_in_conv2, c_out_conv2, k_conv2, _ = w_conv2_dnn.size()\n",
    "    \n",
    "    mu_bnn_conv1 = bnn_multi.conv1.mu_kernel\n",
    "    mu_flat_conv1 = mu_bnn_conv1.view(-1)\n",
    "    \n",
    "    mu_bnn_conv2 = bnn_multi.conv2.mu_kernel\n",
    "    mu_flat_conv2 = mu_bnn_conv2.view(-1)\n",
    "    \n",
    "    w_flat_conv1 = w_conv1_dnn.view(-1)\n",
    "    w_flat_conv2 = w_conv2_dnn.view(-1)\n",
    "\n",
    "    epslion = 1e-5\n",
    "    cov_1 = torch.eye(c_in_conv1 * c_out_conv1 * k_conv1 * k_conv1).to(device)\n",
    "    cov_2 = torch.eye(c_in_conv2 * c_out_conv2 * k_conv2 * k_conv2).to(device)\n",
    "    \n",
    "    # Sampling the weight\n",
    "    w_bnn_conv1 = torch.distributions.MultivariateNormal(mu_flat_conv1, cov_1).rsample().reshape(c_in_conv1, c_out_conv1, k_conv1, k_conv1)\n",
    "    w_bnn_conv2 = torch.distributions.MultivariateNormal(mu_flat_conv2, cov_2).rsample().reshape(c_in_conv2, c_out_conv2, k_conv2, k_conv2)\n",
    "\n",
    "    nll = (w_bnn_conv1 - w_conv1_dnn).pow(2).mean()\n",
    "    nll += (w_bnn_conv2 - w_conv2_dnn).pow(2).mean()\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    nll.backward()  # Do not use retain_graph=True\n",
    "    optimizer.step()\n",
    "\n",
    "    mu_log.append(mu_flat_conv1.mean().item() + mu_flat_conv2.mean().item())\n",
    "    \n",
    "    losses.append(nll.item())\n",
    "    \n",
    "    mean_loss.append(np.mean(losses))\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "# plt.subplot(1,3,1)\n",
    "# plt.imshow(cov_not_optimized[:9,:9])\n",
    "# plt.title('Covariance matrix (not optimized)')\n",
    "\n",
    "# plt.subplot(1,3,2)\n",
    "# i=1\n",
    "# plt.imshow(bnn_multi.conv1.get_covariance_matrix().cpu().detach().numpy()[:,:][:9*i,:9*i])\n",
    "# plt.title('Covariance matrix (optimized)')\n",
    "# plt.colorbar()\n",
    "# plt.subplot(1,3,3)\n",
    "plt.plot(mean_loss[:])\n",
    "plt.grid()\n",
    "plt.title('Loss')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "bnn_with_good_prior = LeNet_BNN().to(device) \n",
    "\n",
    "'''\n",
    "bnn_with_good_prior.conv1.prior_mean = bnn.conv1.mu_kernel.detach().clone().flatten()\n",
    "bnn_with_good_prior.conv1.prior_variance = bnn_with_good_prior.conv1.get_covariance_matrix(bnn.conv1.L_param.detach().clone(), bnn.conv1.B_param.detach().clone())\n",
    "\n",
    "    epslion = 1e-6\n",
    "    cov_1 = bnn_multi.conv1.get_covariance_matrix() + epslion * torch.eye(c_in_conv1 * c_out_conv1 * k_conv1 * k_conv1).to(device)\n",
    "    cov_2 = bnn_multi.conv2.get_covariance_matrix() + epslion * torch.eye(c_in_conv2 * c_out_conv2 * k_conv2 * k_conv2).to(device)\n",
    "    \n",
    "'''\n",
    "epslion = 1e-5\n",
    "\n",
    "bnn_with_good_prior.conv1.prior_mean = bnn_multi.conv1.mu_kernel.detach().clone().flatten()\n",
    "bnn_with_good_prior.conv1.prior_variance = torch.eye(c_in_conv1 * c_out_conv1 * k_conv1 * k_conv1).to(device)\n",
    "\n",
    "bnn_with_good_prior.conv2.prior_mean = bnn_multi.conv2.mu_kernel.detach().clone().flatten()\n",
    "bnn_with_good_prior.conv2.prior_variance = torch.eye(c_in_conv2 * c_out_conv2 * k_conv2 * k_conv2).to(device)\n",
    "\n",
    "bnn_with_good_prior.conv1.mu_kernel = bnn_multi.conv1.mu_kernel\n",
    "# bnn_with_good_prior.conv1.L_param = bnn_multi.conv1.L_param\n",
    "\n",
    "bnn_with_good_prior.conv2.mu_kernel = bnn_multi.conv2.mu_kernel\n",
    "# bnn_with_good_prior.conv2.L_param = bnn_multi.conv2.L_param\n",
    "\n",
    "bnn_with_good_prior.fc1 = dnn.fc1\n",
    "\n",
    "plt.subplot(1,3,1)\n",
    "plt.imshow(bnn_with_good_prior.conv1.prior_variance.cpu().detach().numpy())\n",
    "# plt.colorbar()\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "plt.imshow(bnn_with_good_prior.conv2.prior_variance.cpu().detach().numpy())\n",
    "\n",
    "# plt.subplot(1,3,3)\n",
    "# plt.imshow(bnn_with_good_prior.fc1.prior_variance.cpu().detach().numpy())\n",
    "# plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class args:\n",
    "    pass\n",
    "args = args()\n",
    "args.t = 1.0\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter(f'runs/bnn_with_covI_multi_new_implementation_1e-5')\n",
    "train_BNN(epoch= 100, model = bnn_with_good_prior.cuda(), train_loader= train_loader, test_loader= test_loader, optimizer= optim.Adam(bnn_with_good_prior.parameters(), lr=1e-3), writer = writer, mc_runs = 100, bs = 1024, device = 'cuda', args=args, moped=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomly Initialized Cov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "bnn_multi = LeNet_BNN().to(device)\n",
    "optimizer = optim.Adam([bnn_multi.conv1.mu_kernel, bnn_multi.conv2.mu_kernel], lr=1e-3)\n",
    "\n",
    "mu_log = []\n",
    "losses = []\n",
    "mean_loss = []\n",
    "for idx, _ in tqdm(enumerate(range(10000))):\n",
    "    w_conv1_dnn = dnn.conv1.weight.data.to(device)\n",
    "    w_conv2_dnn = dnn.conv2.weight.data.to(device)\n",
    "    \n",
    "    c_in_conv1, c_out_conv1, k_conv1, _ = w_conv1_dnn.size()\n",
    "    c_in_conv2, c_out_conv2, k_conv2, _ = w_conv2_dnn.size()\n",
    "    \n",
    "    mu_bnn_conv1 = bnn_multi.conv1.mu_kernel\n",
    "    mu_flat_conv1 = mu_bnn_conv1.view(-1)\n",
    "    \n",
    "    mu_bnn_conv2 = bnn_multi.conv2.mu_kernel\n",
    "    mu_flat_conv2 = mu_bnn_conv2.view(-1)\n",
    "    \n",
    "    w_flat_conv1 = w_conv1_dnn.view(-1)\n",
    "    w_flat_conv2 = w_conv2_dnn.view(-1)\n",
    "\n",
    "    epslion = 1e-3\n",
    "    cov_1 = bnn_multi.conv1.get_covariance_matrix()\n",
    "    cov_2 = bnn_multi.conv2.get_covariance_matrix()\n",
    "    \n",
    "    # Sampling the weight\n",
    "    w_bnn_conv1 = torch.distributions.MultivariateNormal(mu_flat_conv1, cov_1).rsample().reshape(c_in_conv1, c_out_conv1, k_conv1, k_conv1)\n",
    "    w_bnn_conv2 = torch.distributions.MultivariateNormal(mu_flat_conv2, cov_2).rsample().reshape(c_in_conv2, c_out_conv2, k_conv2, k_conv2)\n",
    "\n",
    "    nll = (w_bnn_conv1 - w_conv1_dnn).pow(2).mean()\n",
    "    nll += (w_bnn_conv2 - w_conv2_dnn).pow(2).mean()\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    nll.backward()  # Do not use retain_graph=True\n",
    "    optimizer.step()\n",
    "\n",
    "    mu_log.append(mu_bnn_conv1.mean().item() + mu_bnn_conv2.mean().item())\n",
    "    \n",
    "    losses.append(nll.item())\n",
    "    \n",
    "    mean_loss.append(np.mean(losses))\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "# plt.subplot(1,3,1)\n",
    "# plt.imshow(cov_not_optimized[:9,:9])\n",
    "# plt.title('Covariance matrix (not optimized)')\n",
    "\n",
    "# plt.subplot(1,3,2)\n",
    "# i=1\n",
    "# plt.imshow(bnn_multi.conv1.get_covariance_matrix().cpu().detach().numpy()[:,:][:9*i,:9*i])\n",
    "# plt.title('Covariance matrix (optimized)')\n",
    "# plt.colorbar()\n",
    "# plt.subplot(1,3,3)\n",
    "plt.plot(mean_loss[:])\n",
    "plt.grid()\n",
    "plt.title('Loss')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "bnn_with_good_prior = LeNet_BNN().to(device) \n",
    "\n",
    "'''\n",
    "bnn_with_good_prior.conv1.prior_mean = bnn.conv1.mu_kernel.detach().clone().flatten()\n",
    "bnn_with_good_prior.conv1.prior_variance = bnn_with_good_prior.conv1.get_covariance_matrix(bnn.conv1.L_param.detach().clone(), bnn.conv1.B_param.detach().clone())\n",
    "\n",
    "    epslion = 1e-6\n",
    "    cov_1 = bnn_multi.conv1.get_covariance_matrix() + epslion * torch.eye(c_in_conv1 * c_out_conv1 * k_conv1 * k_conv1).to(device)\n",
    "    cov_2 = bnn_multi.conv2.get_covariance_matrix() + epslion * torch.eye(c_in_conv2 * c_out_conv2 * k_conv2 * k_conv2).to(device)\n",
    "    \n",
    "'''\n",
    "epslion = 1e-3\n",
    "\n",
    "bnn_with_good_prior.conv1.prior_mean = bnn_multi.conv1.mu_kernel.detach().clone().flatten()\n",
    "bnn_with_good_prior.conv1.prior_variance = bnn_multi.conv1.get_covariance_matrix().detach().clone()\n",
    "\n",
    "bnn_with_good_prior.conv2.prior_mean = bnn_multi.conv2.mu_kernel.detach().clone().flatten()\n",
    "bnn_with_good_prior.conv2.prior_variance = bnn_multi.conv2.get_covariance_matrix().detach().clone()\n",
    "\n",
    "bnn_with_good_prior.conv1.mu_kernel = bnn_multi.conv1.mu_kernel\n",
    "# bnn_with_good_prior.conv1.L_param = bnn_multi.conv1.L_param\n",
    "\n",
    "bnn_with_good_prior.conv2.mu_kernel = bnn_multi.conv2.mu_kernel\n",
    "# bnn_with_good_prior.conv2.L_param = bnn_multi.conv2.L_param\n",
    "\n",
    "bnn_with_good_prior.fc1 = dnn.fc1\n",
    "\n",
    "plt.subplot(1,3,1)\n",
    "plt.imshow(bnn_with_good_prior.conv1.prior_variance.cpu().detach().numpy())\n",
    "# plt.colorbar()\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "plt.imshow(bnn_with_good_prior.conv2.prior_variance.cpu().detach().numpy())\n",
    "\n",
    "# plt.subplot(1,3,3)\n",
    "# plt.imshow(bnn_with_good_prior.fc1.prior_variance.cpu().detach().numpy())\n",
    "# plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class args:\n",
    "    pass\n",
    "args = args()\n",
    "args.t = 1.0\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter(f'runs/bnn_with_cov_random_multi_new_implementation')\n",
    "train_BNN(epoch= 100, model = bnn_with_good_prior.cuda(), train_loader= train_loader, test_loader= test_loader, optimizer= optim.Adam(bnn_with_good_prior.parameters(), lr=1e-3), writer = writer, mc_runs = 100, bs = 1024, device = 'cuda', args=args, moped=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample the weights\n",
    "\n",
    "# loss = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam([bnn_multi.conv1.mu_kernel, bnn_multi.conv1.L_param, bnn_multi.conv1.B_param], lr=1e-3)\n",
    "\n",
    "# Log for plotting\n",
    "mu_log = []\n",
    "sigma_log = []\n",
    "losses = []\n",
    "for epoch in range(1000):\n",
    "    \n",
    "    mu = bnn_multi.conv1.mu_kernel\n",
    "    sigma = bnn_multi.conv1.get_covariance_matrix(bnn_multi.conv1.L_param, bnn_multi.conv1.B_param)\n",
    "\n",
    "    k = \n",
    "    loss_val = 0.5 * (k * torch.log(2 * torch.pi) + torch.log(cov_det) + mahalanobis_dist)\n",
    "    # loss_val = 0.5 * torch.mean((mu - w_conv1_dnn)**2 / sigma + sigma )\n",
    "        \n",
    "        \n",
    "    \n",
    "    # loss_val = loss(w, w_conv1_dnn)\n",
    "    optimizer.zero_grad()\n",
    "    loss_val.backward()\n",
    "    optimizer.step()\n",
    "    mu_log.append(mu.mean().item())\n",
    "    sigma_log.append(sigma.mean().item())\n",
    "    losses.append(loss_val.item())\n",
    "    \n",
    "# plot training progress\n",
    "plt.plot(losses)\n",
    "plt.plot(mu_log)\n",
    "plt.plot(sigma_log)\n",
    "plt.grid()\n",
    "plt.legend(['loss', 'mu', 'sigma'])\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "\n",
    "# 예시 데이터, 실제 데이터로 대체하세요.\n",
    "mu_kernel = bnn_uni.conv1.mu_kernel  # 실제 mu_kernel 값으로 대체\n",
    "rho_kernel = bnn_uni.conv1.rho_kernel  # 실제 rho_kernel 값으로 대체\n",
    "w_dnn = dnn.conv1.weight.data\n",
    "# sigma_weight 계산\n",
    "sigma_weight = torch.log1p(torch.exp(rho_kernel))\n",
    "\n",
    "plt.figure(figsize=(15,15))\n",
    "for idx, (w_target, mu, sigma) in enumerate(zip(w_dnn, mu_kernel, sigma_weight)):\n",
    "    \n",
    "    print(w_target.shape, mu.shape, sigma.shape)\n",
    "    w_target = w_target.cpu().detach().numpy()\n",
    "    plt.subplot(6,1, idx+1)\n",
    "    for i in range(3):\n",
    "        \n",
    "        for j in range(3):\n",
    "            \n",
    "            plt.axvline(x=w_target[0,i,j], color='r', linestyle='--', label='target')\n",
    "            \n",
    "            x = np.linspace(mu[0,i,j].item() - 3 * sigma[0,i,j].item(), mu[0,i,j].item() + 3 * sigma[0,i,j].item(), 1000)\n",
    "            pdf = norm.pdf(x, loc=mu[0,i,j].item(), scale= sigma[0,i,j].item())\n",
    "            # plt.subplot(3,3,i+j+1)\n",
    "\n",
    "            plt.plot(x, pdf, label=f'N({mu[0,i,j].item():.2f}, {sigma[0,i,j].item():.2f})')\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bayesian_torch.layers.variational_layers.conv_variational_layers import Conv2dReparameterization_Multivariate\n",
    "\n",
    "# Hook 함수 정의\n",
    "def hook_fn(module, input, output):\n",
    "    if isinstance(module, Conv2dReparameterization_Multivariate):\n",
    "        if module == bnn.conv1:\n",
    "            bnn_conv1_out.append(output)\n",
    "        elif module == bnn.conv2:\n",
    "            bnn_conv2_out.append(output)\n",
    "    # elif isinstance(module, nn.AvgPool2d):\n",
    "    #     bnn_pool_out.append(output)\n",
    "\n",
    "\n",
    "# 모델 초기화\n",
    "# dnn = LeNet()\n",
    "bnn = LeNet_BNN()  # 실제로는 Bayesian Neural Network여야 함\n",
    "\n",
    "# Hook을 저장할 리스트\n",
    "dnn_conv1_out = []\n",
    "dnn_conv2_out = []\n",
    "dnn_pool_out = []\n",
    "bnn_conv1_out = []\n",
    "bnn_conv2_out = []\n",
    "bnn_pool_out = []\n",
    "\n",
    "# Hook 등록\n",
    "dnn.conv1.register_forward_hook(lambda m, i, o: dnn_conv1_out.append(o))\n",
    "dnn.conv2.register_forward_hook(lambda m, i, o: dnn_conv2_out.append(o))\n",
    "dnn.pool.register_forward_hook(lambda m, i, o: dnn_pool_out.append(o))\n",
    "\n",
    "bnn.conv1.register_forward_hook(hook_fn)\n",
    "bnn.conv2.register_forward_hook(hook_fn)\n",
    "bnn.pool.register_forward_hook(hook_fn)\n",
    "\n",
    "# 옵티마이저 설정\n",
    "optimizer = torch.optim.Adam(bnn.parameters(), lr=1e-3)#, momentum=0.9, nesterov=True)\n",
    "\n",
    "# Knowledge Distillation 학습 루프\n",
    "epochs = 10\n",
    "for e in range(epochs):\n",
    "    pbar = tqdm(enumerate(train_loader))\n",
    "    dnn.eval().cuda()\n",
    "    bnn.train().cuda()\n",
    "    \n",
    "    losses = []\n",
    "    for batch_idx, (data, target) in pbar:\n",
    "        data, target = data.cuda(), target.cuda()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            dnn_conv1_out.clear()\n",
    "            dnn_conv2_out.clear()\n",
    "            dnn_pool_out.clear()\n",
    "            \n",
    "            y_t = dnn(data)\n",
    "            # output_t = [dnn_conv1_out[0], dnn_conv2_out[0], y_t]\n",
    "            output_t = [dnn_conv1_out[0]]\n",
    "            \n",
    "        bnn_conv1_out.clear()\n",
    "        bnn_conv2_out.clear()\n",
    "        bnn_pool_out.clear()\n",
    "        \n",
    "        y_s, _ = bnn(data)\n",
    "        # output_s = [bnn_conv1_out[0][0], bnn_conv2_out[0][0], y_s]\n",
    "        output_s = [bnn_conv1_out[0][0]]\n",
    "        \n",
    "        loss = 0\n",
    "        for idx, (t, s) in enumerate(zip(output_t, output_s)):\n",
    "            loss += F.mse_loss(s, t)\n",
    "        \n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        pbar.set_description(f\"Loss: {np.mean(losses):.3f} Epoch: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the covariance matrix\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "conv1_cov = bnn.conv1.get_covariance_matrix(bnn.conv1.L_param, bnn.conv1.B_param).cpu().detach().numpy()\n",
    "# conv2_cov = bnn.conv2.get_covariance_matrix(bnn.conv2.L_param, bnn.conv2.B_param).cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=1\n",
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(conv1_cov[:9*i,:9*i], cmap='plasma')\n",
    "\n",
    "i=9\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(conv1_cov[:9*i,:9*i], cmap='plasma')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "bnn_with_good_prior = LeNet_BNN()\n",
    "bnn_with_good_prior.conv1.prior_mean = bnn.conv1.mu_kernel.detach().clone().flatten()\n",
    "bnn_with_good_prior.conv1.prior_variance = bnn_with_good_prior.conv1.get_covariance_matrix(bnn.conv1.L_param.detach().clone(), bnn.conv1.B_param.detach().clone())\n",
    "\n",
    "# bnn_with_good_prior.conv2.prior_mean = bnn.conv2.mu_kernel.detach().clone().flatten()\n",
    "# bnn_with_good_prior.conv2.prior_variance = bnn_with_good_prior.conv2.get_covariance_matrix(bnn.conv2.L_param.detach().clone(), bnn.conv2.B_param.detach().clone())\n",
    "\n",
    "train_BNN(10, bnn_with_good_prior)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the covariance matrix\n",
    "import matplotlib.pyplot as plt\n",
    "conv1_cov = naiive_bnn.conv1.get_covariance_matrix(naiive_bnn.conv1.L_param, naiive_bnn.conv1.B_param).cpu().detach().numpy()\n",
    "\n",
    "plt.figure(figsize=(15,15))\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(conv1_cov[:,:], cmap='plasma')\n",
    "\n",
    "conv1_cov2 = bnn_with_good_prior.conv1.get_covariance_matrix(bnn_with_good_prior.conv1.L_param, bnn_with_good_prior.conv1.B_param).cpu().detach().numpy()\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(conv1_cov2[:,:], cmap='plasma')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
